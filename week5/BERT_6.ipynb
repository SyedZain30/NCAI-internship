{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCRm-QecJahz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.preprocessing \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import wordcloud\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split \n",
        "from keras.utils import np_utils\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "sns.set()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yztVnxaiP1aa",
        "outputId": "48e2423b-654f-4d90-85ea-c603d96751b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5qdaWTOUQ-g",
        "outputId": "1f4714cf-bbd9-483d-a0e8-b316ddbf7c17"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HidzBFgoP2eB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xF5IZih_SGDz",
        "outputId": "52e590de-2d9c-4635-b162-92bcdff5d79d"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TUKL summer internship/week 2/Supreme Court Data 5.csv')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Case Files</th>\n",
              "      <th>Label</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017 P T D 1481\\n[Supreme Court of Pakistan]\\n...</td>\n",
              "      <td>Non-Bailable</td>\n",
              "      <td>Against Federation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015 S C M R 365\\n[Supreme Court of Pakistan]\\...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>illegal appointments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016 S C M R 1420\\n[Supreme Court of Pakistan]...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>Theft/Fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016 S C M R 662\\n[Supreme Court of Pakistan]\\...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>Theft/Fraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P L D 2012 Supreme Court 649\\nPresent: Mian Sh...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>illegal appointments</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Case Files  ...              Category\n",
              "0  2017 P T D 1481\\n[Supreme Court of Pakistan]\\n...  ...    Against Federation\n",
              "1  2015 S C M R 365\\n[Supreme Court of Pakistan]\\...  ...  illegal appointments\n",
              "2  2016 S C M R 1420\\n[Supreme Court of Pakistan]...  ...           Theft/Fraud\n",
              "3  2016 S C M R 662\\n[Supreme Court of Pakistan]\\...  ...           Theft/Fraud\n",
              "4  P L D 2012 Supreme Court 649\\nPresent: Mian Sh...  ...  illegal appointments\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tYcUOHZJ9Kb",
        "outputId": "9147e30f-5aad-4865-b039-674eb0d89880"
      },
      "source": [
        "df1 = df[['Case Files','Label']]\n",
        "df1.isnull().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Case Files    0\n",
              "Label         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqY0yXHhKWfg",
        "outputId": "b24a9316-476c-416a-95f8-046853966588"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5nvmSVmKbsc"
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTwtorxdJ9zi",
        "outputId": "13e99c5e-6e79-4e78-b79a-825069f2bdc3"
      },
      "source": [
        "def cleaning(df, stop_words):\n",
        "\n",
        "    df['Case Files'] = df['Case Files'].apply(lambda x: ' '.join(x.lower() for x in x.split())) \n",
        "\n",
        "                               \n",
        "\n",
        "    # Replacing the digits/numbers\n",
        "\n",
        "    df['Case Files'] = df['Case Files'].str.replace('d', '')\n",
        "\n",
        "            \n",
        "\n",
        "    # Removing stop words\n",
        "\n",
        "    df['Case Files'] = df['Case Files'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
        "\n",
        "    #lower casing\n",
        "    df['Case Files'] = df['Case Files'].str.lower()\n",
        "\n",
        "    #punctuation removal         \n",
        "    df[\"Case Files\"] = df[\"Case Files\"].apply(lambda text: remove_punctuation(text))\n",
        "    # Lemmatization\n",
        "\n",
        "    df['Case Files'] = df['Case Files'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
        "\n",
        "            \n",
        "\n",
        "    return df\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "df1 = cleaning(df1, stop_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oGsCDBfZKgwE",
        "outputId": "6bbcefec-b02b-4800-eee8-d170d7a2d562"
      },
      "source": [
        "df2 = df1.rename(columns={'Case Files': 'Case_Files'})\n",
        "df2.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Case_Files</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017 p 1481 supreme court pakistan present mia...</td>\n",
              "      <td>Non-Bailable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015 c r 365 supreme court pakistan present na...</td>\n",
              "      <td>Bailable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016 c r 1420 supreme court pakistan present a...</td>\n",
              "      <td>Bailable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016 c r 662 supreme court pakistan present an...</td>\n",
              "      <td>Bailable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p l 2012 supreme court 649 present mian shakir...</td>\n",
              "      <td>Bailable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Case_Files         Label\n",
              "0  2017 p 1481 supreme court pakistan present mia...  Non-Bailable\n",
              "1  2015 c r 365 supreme court pakistan present na...      Bailable\n",
              "2  2016 c r 1420 supreme court pakistan present a...      Bailable\n",
              "3  2016 c r 662 supreme court pakistan present an...      Bailable\n",
              "4  p l 2012 supreme court 649 present mian shakir...      Bailable"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99n4EgqeKhNL"
      },
      "source": [
        "#tokenizer = Tokenizer(num_words=500, split=' ') \n",
        "#tokenizer.fit_on_texts(df2['Case_Files'].values)\n",
        "#X = tokenizer.texts_to_sequences(df2['Case_Files'].values)\n",
        "#X = pad_sequences(X)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "576ozfY_KhqS",
        "outputId": "420554cf-649f-41be-8032-4c2903d8abbb"
      },
      "source": [
        "df2['encoded']=df2['Label'].apply(lambda x: 1 if x=='Bailable' else 0)\n",
        "df2.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Case_Files</th>\n",
              "      <th>Label</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017 p 1481 supreme court pakistan present mia...</td>\n",
              "      <td>Non-Bailable</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015 c r 365 supreme court pakistan present na...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016 c r 1420 supreme court pakistan present a...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016 c r 662 supreme court pakistan present an...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p l 2012 supreme court 649 present mian shakir...</td>\n",
              "      <td>Bailable</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Case_Files         Label  encoded\n",
              "0  2017 p 1481 supreme court pakistan present mia...  Non-Bailable        0\n",
              "1  2015 c r 365 supreme court pakistan present na...      Bailable        1\n",
              "2  2016 c r 1420 supreme court pakistan present a...      Bailable        1\n",
              "3  2016 c r 662 supreme court pakistan present an...      Bailable        1\n",
              "4  p l 2012 supreme court 649 present mian shakir...      Bailable        1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDQSOcpLbP1",
        "outputId": "7cf2bbd3-563b-49da-bf76-3532f1994d07"
      },
      "source": [
        "df2['encoded']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0\n",
              "1     1\n",
              "2     1\n",
              "3     1\n",
              "4     1\n",
              "     ..\n",
              "61    1\n",
              "62    0\n",
              "63    0\n",
              "64    1\n",
              "65    1\n",
              "Name: encoded, Length: 66, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gLqzxGU7yg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split#stratify ensures balance between spam and no spam in test and train\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df2['Case_Files'], df2['encoded'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df2['encoded'])\n",
        "\n",
        "# we will use temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZH-xHRz1PPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a056db5d-1421-4661-875c-2102c4b50691"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdtcQr_UVFvj"
      },
      "source": [
        "# sample data\n",
        "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWprVhScVKxR",
        "outputId": "6822ae37-0070-446a-c5a7-1c8c4f5804b9"
      },
      "source": [
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "OWTkNk9sVM4f",
        "outputId": "03019ee7-0f9e-45cc-9166-f473426e9563"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fded593ec50>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJF1rGrzVQAo"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX_XjnWHVVrw",
        "outputId": "a3db0b2c-2b41-4030-a88d-c48504bd42c1"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7pYst6_VXvn"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEy5oZonWdxs"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du2lNIL2WfPy"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nxq42aCWhY_"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask,return_dict=False):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsbRXClBWim2"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nIXv5rUWkiQ"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKPKaFdFWqia",
        "outputId": "614db4d4-1fe3-4c3d-f3d1-8ef302a31e7e"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.4375     0.76666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5FLNA7HWs3p"
      },
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4jyD0MZWxuZ"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "304rShdrW0Dm"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pizq1Rk4W20Q",
        "outputId": "2c8c0f3a-cae8-4632-8ea0-b29ed526ba96"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.086\n",
            "Validation Loss: 0.976\n",
            "\n",
            " Epoch 2 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.904\n",
            "Validation Loss: 0.742\n",
            "\n",
            " Epoch 3 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.693\n",
            "Validation Loss: 0.853\n",
            "\n",
            " Epoch 4 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.729\n",
            "Validation Loss: 0.744\n",
            "\n",
            " Epoch 5 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.662\n",
            "Validation Loss: 0.705\n",
            "\n",
            " Epoch 6 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.706\n",
            "Validation Loss: 0.714\n",
            "\n",
            " Epoch 7 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.695\n",
            "Validation Loss: 0.727\n",
            "\n",
            " Epoch 8 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.682\n",
            "Validation Loss: 0.770\n",
            "\n",
            " Epoch 9 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.673\n",
            "Validation Loss: 0.734\n",
            "\n",
            " Epoch 10 / 10\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.699\n",
            "Validation Loss: 0.717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSiWPCNxYjN1",
        "outputId": "ffe09db1-490a-4d10-8452-a5d531ba09a1"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bTXcA-jYlb-"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1xbnPZ7gfEQ",
        "outputId": "9216c7ee-804a-49f9-8d12-0f9e35597a92"
      },
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.25      0.40         4\n",
            "           1       0.67      1.00      0.80         6\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.83      0.62      0.60        10\n",
            "weighted avg       0.80      0.70      0.64        10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNuypc88hHve",
        "outputId": "4fafdc48-cf4c-4afe-afc1-d5df931e42ff"
      },
      "source": [
        "accuracy_score(test_y,preds)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "DzYy87Jmgfmn",
        "outputId": "1fa1f3ba-b7d9-445d-fecc-952df9fc9353"
      },
      "source": [
        "# confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0  0  1\n",
              "row_0      \n",
              "0      1  3\n",
              "1      0  6"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRBgxwelggDX"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}